import requests
import csv
from bs4 import BeautifulSoup

def get_data():
    url = "https://finance.naver.com/item/sise_day.nhn" # 네이버 금융 일별시세 사이트
    headers = {""} # 구글에 what is my user agent 검색하여 입력 

    title = ("날짜	종가	전일비	시가	고가	저가	거래량".split("\t"))
    name_code_dict = {"삼성전자": "005930", "SK하이닉스": "000660", "LG화학": "051910", "삼성전자우": "005935", "NAVER": "035420", "삼성바이오로직스": "207940", "카카오": "035720", "현대차": "005380", "삼성SDI": "006400", "셀트리온": "068270"}

    for code in name_code_dict:
        # 엑셀파일 만들기(.csv)
        filename = "{}.csv".format(code)
        f = open(filename, "w", encoding="utf-8-sig", newline="")
        writer = csv.writer(f)
        writer.writerow(title)

        # 마지막 페이지 찾기
        res = requests.get("{}?code={}".format(url, name_code_dict[code]), headers = headers)
        html = BeautifulSoup(res.text, 'lxml') 
        pgrr = html.find('td', class_='pgRR') 
        s = str(pgrr.a['href']).split('=')
        last_page = s[-1]

        # 각각의 페이지 BeautifulSoup 실행
        for page in range(1, int(last_page)+1):   
            page_url = "{}?code={}&page={}".format(url, name_code_dict[code], page)
            res = requests.get(page_url, headers = headers)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "lxml")

            date_rows = soup.find("table", attrs={"class":"type2"}).find_all("tr")

            # 정보를 저장
            for row in date_rows:
                columns = row.find_all("td")
                if len(columns)<=1:
                    continue
                data = [column.get_text().strip() for column in columns]
                writer.writerow(data)

        print("{} 생성완료".format(code))
